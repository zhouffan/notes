### 1、概念

[Typora添加数学公式](https://blog.csdn.net/mingzhuo_126/article/details/82722455)



**数据结构**就是指一组数据的存储结构。**算法**就是操作数据的一组方法

数据结构是为算法服务的，算法要作用在特定的数据结构之上。

10 个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树；10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。



T(n) = (2n2+2n+3)*unit_time

大 O 时间复杂度表示法： T(n) = O(2n2+2n+3)  

渐进时间复杂度（代码执行时间随数据规模增长的变化趋势）



1. 只关注循环执行次数最多的一段代码

2. 加法法则：总复杂度等于量级最大的那段代码的复杂度

   T1(n)=O(f(n))，T2(n)=O(g(n))；

   那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).

3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

   T1(n)=O(f(n))，T2(n)=O(g(n))；

   那么 T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n)).

多项式量级：

O(2n) 

 O(n!)

非多项式量级：

1. O(1)：时间不随 n 的增大而增长。一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

2. O(logn)、O(nlogn)：对数阶/线性对数阶

   ```java
    i=1;
    while (i <= n)  {
      //每次乘以2
      i = i * 2;
    }
   ```

   每次乘以2，2的多少次方 等于 n， 求出多少（
   $$
   2^x=n
   $$
   ）
   $$
   x=\log_2n
   $$

   $$
   \log_3n = \log_32 * \log_2n
   $$

   O(log3n) = O(C * log2n)

   即 O(Cf(n)) = O(f(n))。

   所以，O(log2n) 就等于 O(log3n).

   忽略对数的“底”，统一表示为 O(logn)。

   

3. O(m+n)、O(m*n)

   ```
   int cal(int m, int n) {
     int sum_1 = 0;
     int i = 1;
     for (; i < m; ++i) {
       sum_1 = sum_1 + i;
     }
   
     int sum_2 = 0;
     int j = 1;
     for (; j < n; ++j) {
       sum_2 = sum_2 + j;
     }
   
     return sum_1 + sum_2;
   }
   ```

   无法事先评估 m 和 n 谁的量级大，时间复杂度就是 O(m+n)

   T1(m) + T2(n) = O(f(m) + g(n))。

   但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))



**空间复杂度分析**：算法的存储空间与数据规模之间的增长关系。

O(1)、O(n)、O(n2 )，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。



最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度。

```java
// n表示数组array的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) {
       pos = i;
       break; //加入此行，则有最好、最差
    }
  }
  return pos;
}
```





### 2、基础

#### 2.1数组

数组（Array）：是一种线性表数据结构。它用一组**连续的**内存空间，来存储一组具有**相同类型**的数据。

**线性表**（Linear List）。顾名思义，线性表就是数据排成**像一条线**一样的结构。每个线性表上的数据最多只有**前和后**两个方向。其实除了**数组，链表、队列、栈**等也是线性表结构。

**非线性表**，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。

`连续的内存空间`和`相同类型`的数据。这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了**保证连续性**，就需要做大量的**数据搬移**工作。

> 计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：
>
> a[i]_address = base_address + i * data_type_size
>
> data_type_size 表示数组中每个元素的大小。数组中存储的是 int 类型数据，所以 data_type_size 就为 4 个字节

> 这里我要特别纠正一个“错误”。我在面试的时候，常常会问数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度 O(1)；数组适合查找，查找时间复杂度为 O(1)”。
>
> 实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为 O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是 O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。

**插入操作：** 需要移动其他下标数据

假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？

如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 **(1+2+…n)/n=O(n)**。

还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。

在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个处理思想在快排中也会用到

**删除操作：**为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是**记录**数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。  （这不就是 JVM 标记清除垃圾回收算法的核心思想吗？）

**性能：数组就会优于容器**

寻址公式：

从0开始： a[k]_address = base_address + k * type_size

从1开始： a[k]_address = base_address + (k-1)*type_size

下标从0开始原因：会多一次计算。及历史原因。



#### 2.2链表

**缓存淘汰的常见**策略有三种：

先进先出策略 FIFO（First In，First Out）、

最少使用策略 LFU（Least Frequently Used）、

最近最少使用策略 LRU（Least Recently Used）。



数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当`内存中没有连续的、足够大的存储空间`时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。

而链表恰恰相反，它并不需要一块连续的内存空间，它通过`“指针”将一组零散的内存块串联起来`使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。

第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。

数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。时间复杂度是 O(1)。

> 链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。
>
> 寻址公式：a[k]_address = base_address + k * type_size

双向链表可以支持 O(1) 时间复杂度的情况下找到**前驱结点**，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效

不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。

**缓存**实际上就是利用了**空间换时间**的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。

数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。

数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。

CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块(这个大小我不太确定。。)并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入。

对于数组来说，存储空间是连续的，所以在加载某个下标的时候可以把以后的几个下标元素也加载到CPU缓存这样执行速度会快于存储空间不连续的链表存储。

如何基于链表实现 LRU 缓存淘汰算法？我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
2. 如果此数据没有在缓存链表中，又可以分为两种情况：
   i. 如果此时缓存未满，则将此结点直接插入到链表的头部；
   ii. 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。

 在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：删除结点中“值等于某个给定值”的结点；删除给定指针指向的结点。对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p->next=q，说明 p 是 q 的前驱结点。但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。



**指针、引用**：都是存储所指对象的内存地址。

插入结点时，一定要注意操作的顺序，要先将结点 x 的 next 指针指向结点 b，再把结点 a 的 next 指针指向结点 x，这样才不会丢失指针，导致内存泄漏。

```
x->next = p->next; // 将x的结点的next指针指向b结点；
p->next = x; // 将p的next指针指向x结点；
```

#### 2.3栈

后进者先出，先进者后出，这就是典型的“栈”结构。

栈是一种“操作受限”的线性表，只允许在**一端**插入和删除数据。

用数组实现的栈，我们叫作**顺序栈**，用链表实现的栈，我们叫作**链式栈**。

```java
// 基于数组实现的顺序栈
public class ArrayStack {
  private String[] items;  // 数组
  private int count;       // 栈中元素个数
  private int n;           //栈的大小

  // 初始化数组，申请一个大小为n的数组空间
  public ArrayStack(int n) {
    this.items = new String[n];
    this.n = n;
    this.count = 0;
  }

  // 入栈操作
  public boolean push(String item) {
    // 数组空间不够了，直接返回false，入栈失败。
    if (count == n) return false;
    // 将item放到下标为count的位置，并且count加一
    items[count] = item;
    ++count;
    return true;
  }

  // 出栈操作
  public String pop() {
    // 栈为空，则直接返回null
    if (count == 0) return null;
    // 返回下标为count-1的数组元素，并且栈中元素个数count减一
    String tmp = items[count-1];
    --count;
    return tmp;
  }
}
```

**操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。**



#### 2.4队列

**先进者先出**，这就是典型的“队列”。

CPU 资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致 CPU 频繁切换，处理性能下降。

同样，用数组实现的队列叫作**顺序队列**，用链表实现的队列叫作**链式队列**。



线程安全的队列我们叫作**并发队列**

**阻塞队列**其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。

这种基于阻塞队列实现的“生产者 - 消费者模型”，可以有效地协调生产和消费的速度。

基于数组的循环队列，利用CAS（Compare And Swap）原子操作. 可以实现一个非常高效的并发队列。 如何实现一个高效的并发队列： 1. 基于数组的循环队列(避免数据搬移) 2. CAS原子操作（避免真正的去OS底层申请锁资源）

一般两种处理策略，**非阻塞策略**，直接拒绝。**阻塞策略**，放入队列进行等待

我们希望公平地处理每个排队的请求，**先进者先服务**，所以队列这种数据结构很适合来存储排队请求。

基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。

而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。

除了前面讲到队列应用在线程池请求排队的场景之外，队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。**(环型指针下移（tail+1）%n)**

队列，特点先进先出，操作一端入队一端出队。 常用实现方式：数组实现顺序队列，链表实现链表队列。 常见结构：并发队列，阻塞队列，环形队列等。(阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全。)



#### 2.4递归

影院里需要知道当前是第几排。（f(n)=f(n-1)+1 其中，f(1)=1）

分解过程，去的过程叫“递”，回来的过程叫“归”。

**写出递推公式，找到终止条件（找到通项公式，找到初始值）**

如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。

写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。递归代码要警惕堆栈溢出

在“栈”那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。**递归容易引起堆栈溢出的风险，如果递归规模大，层次深，容易造成一直在压栈，而系统栈或者说虚拟机栈堆栈空间普遍不大**

通过**控制递归深度**，来避免栈溢出。

```java
// 全局变量，表示递归的深度。
int depth = 0;

int f(int n) {
  ++depth；
  //如果最大深度比较小，比如 10、50，就可以用这种方法，否则这种方法并不是很实用。
  if (depth > 1000) throw exception;

  if (n == 1) return 1;
  return f(n-1) + 1;
}
```

想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。   保存已经计算过的

```java
public int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;

  // hasSolvedList可以理解成一个Map，key是n，value是f(n)
  if (hasSolvedList.containsKey(n)) {
    return hasSolvedList.get(n);
  }

  int ret = f(n-1) + f(n-2);
  hasSolvedList.put(n, ret);
  return ret;
}
```

```java
//第一，如果递归很深，可能会有堆栈溢出的问题。
//第二，如果数据库里存在脏数据，如果 A 的推荐人是 B，B 的推荐人是 C，C 的推荐人是 A，这样就会发生死循环。
//限制递归深度来解决
long findRootReferrerId(long actorId) {
  Long referrerId = select referrer_id from [table] where actor_id = actorId;
  if (referrerId == null) return actorId;
  return findRootReferrerId(referrerId);
}
```





#### 2.5排序

经过某种排序算法排序之后，如果两个 3 的前后顺序没有改变，那我们就把这种排序算法叫作**稳定的排序算法**；如果前后顺序发生变化，那对应的排序算法就叫作**不稳定的排序算法**。

算法稳定性的用处，多次排序中，下一次排序需要依赖上一次排序的稳定结果。比如订单排序中，先按时间排序，再按价格排序，最终要得到同个价格的订单按下单时间排序，就需要算法稳定性了

##### **冒泡排序**

操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。

```java
// 冒泡排序，a表示数组，n表示数组大小
public void bubbleSort(int[] a, int n) {
  if (n <= 1) return;

 for (int i = 0; i < n; ++i) {
    // 提前退出冒泡循环的标志位    没有数据交换，说明是有序的
    boolean flag = false;
    //-i表示，去掉排序好的元素；-1表示跟下一个元素对比，所以不能走到最头上。
    for (int j = 0; j < n - i - 1; ++j) {
      if (a[j] > a[j+1]) { // 交换       j+1
        int tmp = a[j];
        a[j] = a[j+1];
        a[j+1] = tmp;
        flag = true;  // 表示有数据交换      
      }
    }
    if (!flag) break;  // 没有数据交换，提前退出
  }
}
```

冒泡排序是 **原地排序**：只需要常量级空的临时空间。

冒泡排序是 **稳定的排序**： 因为相同元素不会进行交换。

##### **插入排序**

插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。

```java
// 插入排序，a表示数组，n表示数组大小
public void insertionSort(int[] a, int n) {
  if (n <= 1) return;

  for (int i = 1; i < n; ++i) {
    int value = a[i];
    int j = i - 1;  //j在不断减小
    // 查找插入的位置
    for (; j >= 0; --j) {
      if (a[j] > value) {
        a[j+1] = a[j];  // 数据移动
      } else {
        break;
      }
    }
    a[j+1] = value; // 插入数据
  }
}
```

插入排序是原地排序，稳定排序。



##### 选择排序

选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。

是一种**不稳定的**排序算法。选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。

比如 5，8，5，2，9 这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素 2，与第一个 5 交换位置，那**第一个 5 和中间的 5 顺序就变了（前后顺序）**，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就**稍微逊色**了。







#### 总结积累

##### 1 链表和数组的插入删除时间复杂度都是o(n)，为什么链表效率高？（写入比读取慢很多）

问题:
数组在插入删除的时候，要移动元素，复杂度为o(n)。
链表尽管不需要移动元素，只用改变指针关系，但是要插入或删除第i个节点，必须先找到第i-1个节点，复杂度为o(n)。
总体完成一次操作，大家都是o(n)的复杂度，为什么教材和网路上都说，要频繁删除插入操作时，选链表？

答案:
因为这个O(n)内涵不同，分别是写O(n)和读O(n)。
数组擅长读取，链表擅长写入。
写入要先读取定位，再写入。
读取场景：任意序位读取，复杂度： 数组O(1)，链表O(n)。
写入场景：任意序位写入，定位复杂度：数组O(1)，链表O(n)；写入复杂度：数组O(n)，链表O(1)。
在写入场景中，数组链表的复杂度是定位写入复杂度之和，都是O(n),但写入比定位的O(n)慢很多，所以两个表面看起来一样的O(n)的实际时间还是差很多。
所以说链表和数组的插入删除时间复杂度都是o(n)，链表**写入**效率高。 

























